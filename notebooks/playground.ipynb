{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Notebook\n",
    "\n",
    "Simple playground notebook to quickly test idea. Will refine later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import tifffile as tiff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "# TF / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as kl\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_df = pd.read_csv('../data/train_labels_GY1QjFw.csv')\n",
    "train_images_df = pd.read_csv('../data/train_images_Es8kvkp.csv')\n",
    "\n",
    "# Test\n",
    "test_df = pd.read_csv('../data/test_predicted_random_rp2A5Fo.csv')\n",
    "test_images_df = pd.read_csv('../data/test_images_kkwOpBC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.concat([train_images_df.assign(type = 'train'), \n",
    "                      test_images_df.assign(type = 'test')]) \\\n",
    "                        .sort_values(by='sample_id') \\\n",
    "                        .reset_index(drop=True),\n",
    "            color = 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE : remove 0 = clouds ?\n",
    "class_weight = (train_df.drop(columns=['sample_id']).assign(no_data=0).assign(clouds=0).sum(axis=0).sum() / train_df.drop(columns=['sample_id']).assign(no_data=0).assign(clouds = 0).sum(axis=0)) \\\n",
    "    .reset_index()[0] \\\n",
    "    .to_dict()\n",
    "class_weight[0] = 0\n",
    "class_weight[1] = 0\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_handler(path, norm : int = 65536) : \n",
    "    \"\"\"Simple handler\n",
    "    \"\"\"\n",
    "    if norm :\n",
    "        return tiff.imread(path) / norm\n",
    "    else :\n",
    "        return tiff.imread(path).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_handler('../data/dataset/train/images/7089.tif')\n",
    "X.shape, X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_handler('../data/dataset/train/masks/7089.tif', norm = False)\n",
    "Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(i : int) :\n",
    "    \"\"\" Global data handler\"\"\"\n",
    "    return (data_handler(f'../data/dataset/train/images/{i}.tif'),\n",
    "            data_handler(f'../data/dataset/train/masks/{i}.tif', norm = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = handler(7089)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(X[...,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "Simple UNET (quick tests purposes) (shamelessly) inspired from \n",
    "https://github.com/earthcube-lab/challenge-ens/blob/master/framework/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "U-Net model definition.\n",
    "\"\"\"\n",
    "\n",
    "def UNet(input_shape,\n",
    "         num_classes=10,\n",
    "         output_activation='softmax',\n",
    "         num_layers=4):\n",
    "    \"\"\"\n",
    "    Creates a U-Net model (Ronneberger et al 2015)\n",
    "    Architecture adapted from github.com/karolzak/keras-unet/master/keras_unet/models/satellite_unet.py\n",
    "    \"\"\"\n",
    "\n",
    "    def bn_conv_relu(input, filters, **conv2d_kwargs):\n",
    "        x = kl.BatchNormalization()(input)\n",
    "        x = kl.Conv2D(filters, activation='relu', **conv2d_kwargs)(x)\n",
    "        return x\n",
    "\n",
    "    def bn_upconv_relu(input, filters, **conv2d_transpose_kwargs):\n",
    "        x = kl.BatchNormalization()(input)\n",
    "        x = kl.Conv2DTranspose(filters, activation='relu', **conv2d_transpose_kwargs)(x)\n",
    "        return x\n",
    "\n",
    "    inputs = kl.Input(input_shape)\n",
    "\n",
    "    # number of filters in a convolution in the contrastive path (constant)\n",
    "    filters = 16\n",
    "    # number of filters in a convolution in the dilative path (constant)\n",
    "    upconv_filters = 24\n",
    "\n",
    "    conv2d_kwargs = {\n",
    "        'kernel_size': (3,3),\n",
    "        'strides': (1,1),\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': 'he_normal'\n",
    "    }\n",
    "    conv2d_transpose_kwargs = {\n",
    "        'kernel_size': (3,3),\n",
    "        'strides': (2,2),\n",
    "        'padding': 'same',\n",
    "        'output_padding': (1,1)\n",
    "    }\n",
    "    maxpool2d_kwargs = {\n",
    "        'pool_size': (2,2),\n",
    "        'strides': (2,2),\n",
    "        'padding': 'valid',\n",
    "    }\n",
    "\n",
    "    x = kl.Conv2D(filters, activation='relu', **conv2d_kwargs)(inputs)\n",
    "    c1 = bn_conv_relu(x, filters, **conv2d_kwargs)\n",
    "    x = bn_conv_relu(c1, filters, **conv2d_kwargs)\n",
    "    x = kl.MaxPooling2D(**maxpool2d_kwargs)(x)\n",
    "\n",
    "    down_layers = []\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = bn_conv_relu(x, filters, **conv2d_kwargs)\n",
    "        x = bn_conv_relu(x, filters, **conv2d_kwargs)\n",
    "        down_layers.append(x)\n",
    "        x = bn_conv_relu(x, filters, **conv2d_kwargs)\n",
    "        x = kl.MaxPooling2D(**maxpool2d_kwargs)(x)\n",
    "\n",
    "    x = bn_conv_relu(x, filters, **conv2d_kwargs)\n",
    "    x = bn_conv_relu(x, filters, **conv2d_kwargs)\n",
    "    x = bn_upconv_relu(x, filters, **conv2d_transpose_kwargs)\n",
    "\n",
    "    for conv in reversed(down_layers):\n",
    "        x = kl.concatenate([x, conv])\n",
    "        x = bn_conv_relu(x, upconv_filters, **conv2d_kwargs)\n",
    "        x = bn_conv_relu(x, filters, **conv2d_kwargs)\n",
    "        x = bn_upconv_relu(x, filters, **conv2d_transpose_kwargs)\n",
    "\n",
    "    x = kl.concatenate([x, c1])\n",
    "    x = bn_conv_relu(x, upconv_filters, **conv2d_kwargs)\n",
    "    x = bn_conv_relu(x, filters, **conv2d_kwargs)\n",
    "\n",
    "    outputs = kl.Conv2D(num_classes, kernel_size=(1,1), strides=(1,1), activation=output_activation, padding='valid') (x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs], name='unet')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "unet_kwargs = dict(\n",
    "    input_shape=(256, 256, 4),\n",
    "    num_classes=10,\n",
    "    output_activation='softmax',\n",
    "    num_layers=1\n",
    ")\n",
    "print(f\"Creating U-Net with arguments: {unet_kwargs}\")\n",
    "model = UNet(**unet_kwargs)\n",
    "print(\"Summary:\")\n",
    "print(model.summary())\n",
    "\n",
    "input_batch = tf.random.normal((1, 256, 256, 4), name='random_normal_input')\n",
    "output = model(input_batch)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = np.array(train_df.sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = handler(np.random.choice(train_inputs))\n",
    "px.imshow(X[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train_inputs, test_size=0.2, random_state=123)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(input_list, \n",
    "        class_weight : dict = class_weight) : \n",
    "    \"\"\"Simple data generator, based on the handler previously defined. Will allow for data augmentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over the inputs\n",
    "    for i in input_list :\n",
    "\n",
    "        # Get (x,y)\n",
    "        x, y = handler(i)\n",
    "\n",
    "        # Generates weights based on class_weight dict\n",
    "        w = np.vectorize(class_weight.get)(y)\n",
    "\n",
    "        # To categorical\n",
    "        y = tf.keras.utils.to_categorical(y, num_classes=10)\n",
    "\n",
    "        yield((x, y, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  tf.data.Dataset.from_generator(gen, args = [X_train], output_types=(tf.float32, tf.int16, tf.float32))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "test_dataset =  tf.data.Dataset.from_generator(gen, args = [X_test], output_types=(tf.float32, tf.int16, tf.float32))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in train_dataset.take(1) :\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Apply weights\n",
    "\n",
    "According to https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras, \"if tf dataset is used you cannot use the class_weights parameter. Insted return the weight from a parse_function in your pipeline\"\n",
    "\n",
    "TODO Later\n",
    "\n",
    "See also https://stackoverflow.com/questions/65881582/how-to-use-class-weights-in-keras-for-image-segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple optimizers, loss functions and metrics that can be used to compile multi-class segmentation models\n",
    "# Ideally, try different options to get the best accuracy\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['categorical_accuracy'],\n",
    "              sample_weight_mode=\"temporal\",\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FIT !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, \n",
    "          epochs=5, \n",
    "          validation_data = test_dataset, \n",
    "          verbose = 1,\n",
    "          callbacks=[tensorboard_callback],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('hja_py38tf28')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae3519dbe3e22986823bac0e82f2b634b1d8218c22426968656f15ac5c06dcb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
